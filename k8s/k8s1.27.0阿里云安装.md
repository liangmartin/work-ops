k8s1.27.0阿里云安装

# 一、环境配置

```
一个master
两个node

kubernetes 版本1.27.0
containerd-1.6.20
```



## 1.系统配置

### 1.1 修改主机名

```
#master节点
hostnamectl set-hostname master


#node节点
hostnamectl set-hostname node
```



### 1.2 配置hosts

```text
cat >> /etc/hosts << EOF
192.168.33.151 master
192.168.33.152 node
EOF
```



### 1.3 关闭防火墙

```
systemctl stop firewalld
systemctl disable firewalld
```



### 1.4 关闭selinux

```
getenforce
setenforce 0
sed -i "s/SELINUX=enforcing/SELINUX=disabled/g" /etc/selinux/config

```



### 1.5 关闭交换分区

```
sed -i '/swap/s/^/#/' /etc/fstab
swapoff -a
```



### 1.6 文件句柄参数调优

```
vim /etc/security/limits.conf
* soft nofile 65535
* hard nofile 65535
* soft nproc 65535
* hard nproc 65535
```

系统文件优化

```
修改/etc/sysctl.conf添加如下参数

fs.file-max=65535
```



### 1.7 加载内核系统模块

转发 IPv4 并让 iptables 看到桥接流量

```text
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF


modprobe overlay
modprobe br_netfilter


#验证br_netfilter模块
[root@master ~]# lsmod | grep br_netfilter
br_netfilter           22256  0 
bridge                155432  1 br_netfilter
```

配置sysctl参数

```text
# 设置所需的 sysctl 参数，参数在重新启动后保持不变
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
user.max_user_namespaces=28633
EOF


# 应用 sysctl 参数而不重新启动
sudo sysctl --system
```



### 1.8 时间同步

```
安装配置chrony时间同步(推荐)

服务端

#获取本机IP地址
IP=`ip addr | grep 'state UP' -A2 | grep inet | egrep -v '(127.0.0.1|inet6|docker)' | awk '{print $2}' | tr -d "addr:" | head -n 1 | cut -d / -f1`
#安装chrony
yum install -y chrony
#备份配置文件
cp /etc/chrony.conf{,.bak}
#修改配置文件
sed -i '3,6s/^/#/g' /etc/chrony.conf
sed -i "7s|^|server $IP iburst|g" /etc/chrony.conf
echo "allow all" >> /etc/chrony.conf
echo "local stratum 10" >> /etc/chrony.conf
systemctl restart chronyd
systemctl enable chronyd
timedatectl set-ntp true
sleep 5
systemctl restart chronyd
chronyc sources
```





# 二、安装containerd

## 2.1 下载

```
wget https://github.com/containerd/containerd/releases/download/v1.6.20/containerd-1.6.20-linux-amd64.tar.gz
wget https://github.com/containerd/containerd/releases/download/v1.6.20/containerd-1.6.20-linux-amd64.tar.gz.sha256sum


sha256sum containerd-1.6.20-linux-amd64.tar.gz
```

## 2.2 安装

```
2.2 安装
root@jial:~# tar zxvf containerd-1.6.20-linux-amd64.tar.gz -C /usr/local
/bin/
bin/containerd-shim
bin/containerd-shim-runc-v1
bin/containerd-stress
bin/containerd
bin/ctr
bin/containerd-shim-runc-v2


root@jial:~# ls /usr/local/bin/
containerd  containerd-shim  containerd-shim-runc-v1  containerd-shim-runc-v2  containerd-str
```



## 2.3 配置systemd方式启动

通过 systemd 启动 containerd，需要将单元文件从
[https://raw.githubusercontent.com/containerd/containerd/main/containerd.service](https://link.zhihu.com/?target=https%3A//raw.githubusercontent.com/containerd/containerd/main/containerd.service) 下载到 /usr/local/lib/systemd/system/ 目录

```text
wget https://raw.githubusercontent.com/containerd/containerd/main/containerd.service 


mkdir -p /usr/local/lib/systemd/system/
cp containerd.service /usr/local/lib/systemd/system/containerd.service


systemctl daemon-reload
systemctl enable --now containerd
systemctl status containerd.service
```



## 2.4 生成默认配置文件

```text
mkdir -p /etc/containerd
containerd config default > /etc/containerd/config.toml
```



## 2.5 验证

```text
root@jial:~# ctr version
Client:
  Version:  v1.6.20
  Revision: 2806fc1057397dbaeefbea0e4e17bddfbd388f38
  Go version: go1.19.7


Server:
  Version:  v1.6.20
  Revision: 2806fc1057397dbaeefbea0e4e17bddfbd388f38
  UUID: 1aaccbf7-893b-484a-a079-d6017c9c6abf
```



## 2.6 安装crictl

kubernetes中使用crictl管理容器，不使用ctr。

crictl 是 CRI 兼容的容器运行时命令行接口。可以使用它来检查和调试 Kubernetes 节点上的容器运行时和应用程序。

crictl软件包下载地址：Releases · kubernetes-sigs/cri-tools ([http://github.com](https://link.zhihu.com/?target=http%3A//github.com))

```text
VERSION="v1.27.0"
wget https://github.com/kubernetes-sigs/cri-tools/releases/download/$VERSION/crictl-$VERSION-linux-amd64.tar.gz
sudo tar zxvf crictl-$VERSION-linux-amd64.tar.gz -C /usr/local/bin
```

**配置crictl**

```text
crictl config runtime-endpoint unix:///var/run/containerd/containerd.sock
```

**会生成以下配置文件**

```text
[root@master k8s]# cat /etc/crictl.yaml 
runtime-endpoint: "unix:///var/run/containerd/containerd.sock"
image-endpoint: ""
timeout: 0
debug: false
pull-image-on-create: false
disable-pull-on-run: false
```

**验证**

```text
root@jial:~# crictl version
Version:  0.1.0
RuntimeName:  containerd
RuntimeVersion:  v1.6.20
RuntimeApiVersion:  v1
```





## 2.7 安装 runc

从
[https://github.com/opencontainers/runc/releases](https://link.zhihu.com/?target=https%3A//github.com/opencontainers/runc/releases) 下载二进制文件， 验证其SHA256sum，并将其安装为。runc.<ARCH>``/usr/local/sbin/runc

```text
wget https://github.com/opencontainers/runc/releases/download/v1.1.6/runc.amd64
wget https://github.com/opencontainers/runc/releases/download/v1.1.6/runc.sha256sum
grep -r $(sha256sum runc.amd64 |awk '{print $1}') runc.sha256sum


install -m 755 runc.amd64 /usr/local/sbin/runc
```

二进制文件是静态构建的，应该适用于任何 Linux 发行版。



## 2.8 安装 CNI 插件

从
[https://github.com/containernetworking/plugins/releases](https://link.zhihu.com/?target=https%3A//github.com/containernetworking/plugins/releases) 下载存档， 验证其SHA256sum，并将其提取到：cni-plugins-<OS>-<ARCH>-<VERSION>.tgz``/opt/cni/bin

```text
wget https://github.com/containernetworking/plugins/releases/download/v1.2.0/cni-plugins-linux-amd64-v1.2.0.tgz


wget https://github.com/containernetworking/plugins/releases/download/v1.2.0/cni-plugins-linux-amd64-v1.2.0.tgz.sha256


mkdir -p /opt/cni/bin
tar zxvf cni-plugins-linux-amd64-v1.2.0.tgz -C /opt/cni/bin
```



## 2.8 配置阿里云加速器（可选）

```text
#参考：https://github.com/containerd/containerd/blob/main/docs/cri/config.md#registry-configuration
#添加 config_path = "/etc/containerd/certs.d"
sed -i 's/config_path\ =.*/config_path = \"\/etc\/containerd\/certs.d\"/g' /etc/containerd/config.toml
mkdir /etc/containerd/certs.d/docker.io -p


cat > /etc/containerd/certs.d/docker.io/hosts.toml << EOF
server = "https://docker.io"
[host."https://vh3bm52y.mirror.aliyuncs.com"]
  capabilities = ["pull", "resolve"]
EOF


systemctl daemon-reload && systemctl restart containerd
```



## 三、配置cgroup驱动

在 Linux 上，控制组（CGroup）用于限制分配给进程的资源。

kubelet 和底层容器运行时都需要对接控制组 为 Pod 和容器管理资源 ，如 CPU、内存这类资源设置请求和限制。若要对接控制组（CGroup），kubelet 和容器运行时需要使用一个 cgroup 驱动。关键的一点是 **kubelet 和容器运行时需使用相同的 cgroup 驱动**并且采用相同的配置。

```text
#把SystemdCgroup = false修改为：SystemdCgroup = true
sed -i 's/SystemdCgroup\ =\ false/SystemdCgroup\ =\ true/g' /etc/containerd/config.toml


#把sandbox_image = "k8s.gcr.io/pause:3.6"修改为：sandbox_image="registry.aliyuncs.com/google_containers/pause:3.9"


sed -i 's/sandbox_image\ =.*/sandbox_image\ =\ "registry.aliyuncs.com\/google_containers\/pause:3.9"/g' /etc/containerd/config.toml|grep sandbox_image


systemctl daemon-reload 
systemctl restart containerd
```



## 四、安装kubectl kubelet kubeadmin

配置阿里云镜像源

kubernetes镜像*kubernetes下载地址*kubernetes安装教程-阿里巴巴开源镜像站 ([http://aliyun.com](https://link.zhihu.com/?target=http%3A//aliyun.com))

```text
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF


###
yum install -y kubelet kubeadm kubectl
```



启动kubelet服务

```text
systemctl enable kubelet && systemctl start kubelet
```



## 五、拉取镜像到本地

可以访问互联网的这一步可以省略，直接在线安装。

## 5.1 拉取镜像

使用kubeadm config images pull拉取镜像到本地，--image-repository 指定使用的仓库地址，--kubernetes-version 指定集群版本



```text
[root@master containerd]# kubeadm config images pull --image-repository registry.aliyuncs.com/google_containers
W0419 13:29:46.707179    3380 images.go:80] could not find officially supported version of etcd for Kubernetes v1.27.1, falling back to the nearest etcd version (3.5.7-0)
[config/images] Pulled registry.aliyuncs.com/google_containers/kube-apiserver:v1.27.1
[config/images] Pulled registry.aliyuncs.com/google_containers/kube-controller-manager:v1.27.1
[config/images] Pulled registry.aliyuncs.com/google_containers/kube-scheduler:v1.27.1
[config/images] Pulled registry.aliyuncs.com/google_containers/kube-proxy:v1.27.1
[config/images] Pulled registry.aliyuncs.com/google_containers/pause:3.9
[config/images] Pulled registry.aliyuncs.com/google_containers/etcd:3.5.7-0
[config/images] Pulled registry.aliyuncs.com/google_containers/coredns:v1.10.1
```



## 六、配置IPVS

加载ipvs内核模块

```text
cat > /etc/sysconfig/modules/ipvs.modules <<EOF
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
EOF


#ubuntu 为  nf_conntrack
```

执行并检查是否生效

```text
chmod 755 /etc/sysconfig/modules/ipvs.modules && bash /etc/sysconfig/modules/ipvs.modules && lsmod | grep -e ip_vs -e nf_conntrack_ipv4
```

```text
yum install -y ipset ipvsadm


查看规则
# ipvsadm -L -n
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
```

配置kubelet

```text
cat >>  /etc/sysconfig/kubelet << EOF
# KUBELET_CGROUP_ARGS="--cgroup-driver=systemd"
KUBE_PROXY_MODE="ipvs"
EOF
```





## 七、使用kubeadm初始化集群

查看kubeadm版本

```text
# kubeadm version
kubeadm version: &version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.1", GitCommit:"4c9411232e10168d7b050c49a1b59f6df9d7ea4b", GitTreeState:"clean", BuildDate:"2023-04-14T13:20:04Z", GoVersion:"go1.20.3", Compiler:"gc", Platform:"linux/amd64"}
```

生成默认配置文件

```text
kubeadm config print init-defaults > kubeadm.yaml
```

修改相应的配置：

```text
[root@master ~]# cat kubeadm.yaml 
apiVersion: kubeadm.k8s.io/v1beta3
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.33.151  #修改为Master节点IP
  bindPort: 6443
nodeRegistration:
  criSocket: unix:///var/run/containerd/containerd.sock
  imagePullPolicy: IfNotPresent
  name: master  #修改为master节点主机名
  taints: null
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta3
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns: {}
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.aliyuncs.com/google_containers   #修改为阿里镜像仓库
kind: ClusterConfiguration
kubernetesVersion: 1.27.1    #修改为和kubeadm的版本一致
networking:
  dnsDomain: cluster.local
  serviceSubnet: 10.96.0.0/12
  podSubnet: 10.244.0.0/16   ## 设置pod网段
scheduler: {}


#######################################################################
###添加内容：配置kubelet的CGroup为systemd
---
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
cgroupDriver: systemd
```



**使用配置文件初始化集群**

```text
kubeadm init --config kubeadm.yaml
```

或

使用参数初始化文件

```text
kubeadm init  \
    --apiserver-advertise-address=192.168.33.151 \
    --pod-network-cidr=10.244.0.0/16 \
    --image-repository registry.aliyuncs.com/google_containers \
    --kubernetes-version v1.27.1
```

集群初始化后，输出如下信息：

```text
[root@master ~]# kubeadm init --config kubeadm.yaml
[init] Using Kubernetes version: v1.27.1
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
W0419 14:40:22.988535    8656 images.go:80] could not find officially supported version of etcd for Kubernetes v1.27.1, falling back to the nearest etcd version (3.5.7-0)
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local master] and IPs [10.96.0.1 192.168.33.151]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [localhost master] and IPs [192.168.33.151 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [localhost master] and IPs [192.168.33.151 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
W0419 14:40:26.179390    8656 images.go:80] could not find officially supported version of etcd for Kubernetes v1.27.1, falling back to the nearest etcd version (3.5.7-0)
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 8.506536 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node master as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node master as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]
[bootstrap-token] Using token: abcdef.0123456789abcdef
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy


Your Kubernetes control-plane has initialized successfully!


To start using your cluster, you need to run the following as a regular user:


  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config


Alternatively, if you are the root user, you can run:


  export KUBECONFIG=/etc/kubernetes/admin.conf


You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/


Then you can join any number of worker nodes by running the following on each as root:


kubeadm join 192.168.33.151:6443 --token abcdef.0123456789abcdef \
  --discovery-token-ca-cert-hash sha256:76c21fa45295c342a0c9987369257fe38f177a4be65f34bda338c83aac0d15dc
```

删除集群（仅在部署失败时使用）

```text
kubeadm reset
```

## 八、部署Flannel

对于 Kubernetes v1.17+ 使用以下yml文件直接部署，如果使用自定义网段，需要修改podCIDR 10.244.0.0/16

```text
wget --no-check-certificate https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml


# kubectl apply -f  kube-flannel.yml 
namespace/kube-flannel created
serviceaccount/flannel created
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
configmap/kube-flannel-cfg created
daemonset.apps/kube-flannel-ds created
```

查看pod

```text
[root@master k8s]# kubectl get pod -o wide --all-namespaces
NAMESPACE      NAME                             READY   STATUS    RESTARTS   AGE   IP               NODE     NOMINATED NODE   READINESS GATES
kube-flannel   kube-flannel-ds-d2ngt            1/1     Running   0          54s   192.168.33.151   master   <none>           <none>
kube-system    coredns-7bdc4cb885-znxx6         1/1     Running   0          97m   10.244.0.3       master   <none>           <none>
kube-system    coredns-7bdc4cb885-zsxv5         1/1     Running   0          97m   10.244.0.2       master   <none>           <none>
kube-system    etcd-master                      1/1     Running   1          97m   192.168.33.151   master   <none>           <none>
kube-system    kube-apiserver-master            1/1     Running   1          97m   192.168.33.151   master   <none>           <none>
kube-system    kube-controller-manager-master   1/1     Running   1          97m   192.168.33.151   master   <none>           <none>
kube-system    kube-proxy-ktct2                 1/1     Running   0          97m   192.168.33.151   master   <none>           <none>
kube-system    kube-scheduler-master            1/1     Running   1          97m   192.168.33.151   master   <none>           <none>
```



## 九、部署helm

软件包下载地址：Releases · helm/helm ([http://github.com](https://link.zhihu.com/?target=http%3A//github.com))

```text
wget https://get.helm.sh/helm-v3.11.3-linux-amd64.tar.gz
tar zxvf helm-v3.11.3-linux-amd64.tar.gz
cp linux-amd64/helm /usr/local/bin/
```

## 十、添加node节点

查看token

```text
[root@master ~]# kubeadm token list
TOKEN                     TTL         EXPIRES                USAGES                   DESCRIPTION                                                EXTRA GROUPS
abcdef.0123456789abcdef   7h          2023-04-20T06:49:13Z   authentication,signing   <none>                                                     system:bootstrappers:kubeadm:default-node-token
```

token过期后使用kubeadm token create创建， 或使用 kubeadm token create --print-join-command 获取加入集群的命令

```text
kubeadm join 192.168.33.151:6443 --token au3vy4.pznwtrnrkttihhyx --discovery-token-ca-cert-hash sha256:5c6b4c9fff89dcb7d60fe4cf9f5a22fcf5723433453b41039a59e4216f4e5797
```











